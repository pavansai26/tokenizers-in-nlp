{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMn3fv9+4Z3od8hBadRNnTU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavansai26/tokenizers-in-nlp/blob/main/sentence_piece_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SentencePiece is a subword text tokenizer that can be used in natural language processing (NLP) tasks such as machine translation, language modeling, and speech recognition. It was developed by Google and is open source."
      ],
      "metadata": {
        "id": "Vrqz8EqYJhL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SentencePiece is based on the unigram language model, which assigns probabilities to each subword unit based on the frequency of its occurrence in the training corpus. The subword units can be characters, bytes, or a mixture of both, and they are learned jointly with the task-specific model. This allows the model to represent rare words or words that were not present in the training corpus."
      ],
      "metadata": {
        "id": "XSO-vw7JJleQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZe7P7QH_sOH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advantages "
      ],
      "metadata": {
        "id": "bwA0Edg0J4jV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the main advantages of SentencePiece is its flexibility. It can be used with any language and script, and it allows for easy customization of the tokenization process. This is particularly useful for languages with complex morphology, such as agglutinative languages like Turkish or Korean."
      ],
      "metadata": {
        "id": "BYU8GSnTJ9oW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another advantage of SentencePiece is its ability to handle out-of-vocabulary (OOV) words. Since the model learns subword units based on the training corpus, it can represent words that were not present in the corpus by breaking them down into subword units."
      ],
      "metadata": {
        "id": "KGZYfkjTKAsi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mf229ZN-J65a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "disadvantages"
      ],
      "metadata": {
        "id": "dvI8MDd1KDPN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, one potential disadvantage of SentencePiece is that it requires more memory than traditional tokenization methods, as it needs to store the subword vocabulary. Additionally, the tokenization process can be slower, especially during training, as it requires running an additional optimization step to learn the subword units."
      ],
      "metadata": {
        "id": "3o42xqYoKIIU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gQ4mhO6IKEjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "how SentencePiece works"
      ],
      "metadata": {
        "id": "iDrz5CBKKTmK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SentencePiece is a subword tokenizer that works by breaking down words into smaller units called subwords. These subwords can be thought of as building blocks that can be used to represent any word, even if it's not present in the training corpus. This is useful for dealing with rare or unseen words, as well as for languages with complex morphology where words can have many different forms."
      ],
      "metadata": {
        "id": "3YHL2eXLPOk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   First, the tokenizer is trained on a corpus of text. During training, the tokenizer learns a set of subword units and their corresponding frequencies. This is done using an unsupervised learning approach based on the unigram language model, which assigns probabilities to each subword unit based on its frequency in the training corpus.\n",
        "\n",
        "2.   Once the tokenizer has been trained, it can be used to tokenize new text. When a word is encountered that is not in the tokenizer's vocabulary, it is broken down into subword units that are present in the vocabulary. This is done using a greedy algorithm that iteratively selects the longest matching subword unit from the vocabulary.\n",
        "\n"
      ],
      "metadata": {
        "id": "OB4D9h50PRgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, let's say we have the word \"unseen\" and the tokenizer's vocabulary consists of the subwords \"un\", \"seen\", and \"k\". The tokenizer would break down \"unseen\" into \"un\" and \"seen\", because those subwords are present in the vocabulary. The subwords are then represented as separate tokens in the tokenized output."
      ],
      "metadata": {
        "id": "yvgFPgnUPaIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During training or inference, the subwords can be encoded as integers or embeddings and used as input to a machine learning model. This allows the model to handle words that were not present in the training corpus, as well as to represent rare words more effectively."
      ],
      "metadata": {
        "id": "swci67vSPiFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Train tokenizer on a corpus of text\n",
        "def train_tokenizer(data_path, model_prefix, vocab_size):\n",
        "    # Load corpus\n",
        "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        corpus = f.read()\n",
        "\n",
        "    # Compute subwords using unigram language model\n",
        "    subwords = get_subwords(corpus, vocab_size)\n",
        "\n",
        "    # Write subwords to file\n",
        "    with open(f\"{model_prefix}.vocab\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for subword in subwords:\n",
        "            f.write(f\"{subword} {subwords[subword]}\\n\")\n",
        "\n",
        "    # Save tokenizer config\n",
        "    config = {\n",
        "        \"vocab_size\": vocab_size,\n",
        "        \"unk_token\": \"<unk>\",\n",
        "        \"bos_token\": \"<s>\",\n",
        "        \"eos_token\": \"</s>\",\n",
        "        \"pad_token\": \"<pad>\",\n",
        "    }\n",
        "    np.save(f\"{model_prefix}.config\", config)\n",
        "\n",
        "# Load trained tokenizer\n",
        "def load_tokenizer(model_prefix):\n",
        "    # Load subwords and frequencies from file\n",
        "    subwords = {}\n",
        "    with open(f\"{model_prefix}.vocab\", \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            subword, freq = line.strip().split()\n",
        "            subwords[subword] = int(freq)\n",
        "\n",
        "    # Load tokenizer config\n",
        "    config = np.load(f\"{model_prefix}.config\", allow_pickle=True).item()\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = Tokenizer(subwords, config)\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "# Tokenizer class\n",
        "class Tokenizer:\n",
        "    def __init__(self, subwords, config):\n",
        "        self.subwords = subwords\n",
        "        self.config = config\n",
        "        self.id_to_subword = {i: subword for i, subword in enumerate(subwords)}\n",
        "        self.subword_to_id = {subword: i for i, subword in self.id_to_subword.items()}\n",
        "        self.unk_id = self.subword_to_id[self.config[\"unk_token\"]]\n",
        "        self.bos_id = self.subword_to_id[self.config[\"bos_token\"]]\n",
        "        self.eos_id = self.subword_to_id[self.config[\"eos_token\"]]\n",
        "        self.pad_id = self.subword_to_id[self.config[\"pad_token\"]]\n",
        "        self.vocab_size = self.config[\"vocab_size\"]\n",
        "\n",
        "    def tokenize(self, sentence):\n",
        "        # Split sentence into words\n",
        "        words = sentence.strip().split()\n",
        "\n",
        "        # Tokenize each word into subwords\n",
        "        tokens = []\n",
        "        for word in words:\n",
        "            subword_ids = self.get_subword_ids(word)\n",
        "            tokens.extend(subword_ids)\n",
        "\n",
        "        # Add special tokens\n",
        "        tokens = [self.bos_id] + tokens + [self.eos_id]\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def get_subword_ids(self, word):\n",
        "        # Initialize variables\n",
        "        subword_ids = []\n",
        "        start = 0\n",
        "        end = len(word)\n",
        "\n",
        "        while start < end:\n",
        "            # Find the longest matching subword\n",
        "            subword = None\n",
        "            for i in range(end, start, -1):\n",
        "                sub = word[start:i]\n",
        "                if sub in self.subword_to_id:\n",
        "                    subword = sub\n",
        "                    break\n",
        "\n",
        "            # If no matching subword is found, add the unknown token\n",
        "            if subword is None:\n",
        "                subword = self.config[\"unk_token\"]\n",
        "\n",
        "            # Add the subword id to the list of subword ids\n",
        "            subword_id = self.subword_to_id[subword]\n",
        "            subword_ids.append(subword_id)\n",
        "\n"
      ],
      "metadata": {
        "id": "xUPNco4zKUGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class SentencePieceTokenizer:\n",
        "    def __init__(self, vocab_size):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.vocab = None\n",
        "        self.encode_cache = {}\n",
        "\n",
        "    def train(self, data_path):\n",
        "        # Load corpus\n",
        "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            corpus = f.read()\n",
        "\n",
        "        # Compute subwords using unigram language model\n",
        "        subwords = self._get_subwords(corpus, self.vocab_size)\n",
        "\n",
        "        # Build vocabulary\n",
        "        self.vocab = {\"<unk>\": 0, \"<s>\": 1, \"</s>\": 2}\n",
        "        for i, subword in enumerate(subwords):\n",
        "            self.vocab[subword] = i + 3\n",
        "\n",
        "    def encode(self, sentence):\n",
        "        # Check cache\n",
        "        if sentence in self.encode_cache:\n",
        "            return self.encode_cache[sentence]\n",
        "\n",
        "        # Split sentence into words\n",
        "        words = sentence.strip().split()\n",
        "\n",
        "        # Tokenize each word into subwords\n",
        "        tokens = []\n",
        "        for word in words:\n",
        "            subword_ids = self._get_subword_ids(word)\n",
        "            tokens.extend(subword_ids)\n",
        "\n",
        "        # Add special tokens\n",
        "        tokens = [1] + tokens + [2]\n",
        "\n",
        "        # Update cache\n",
        "        self.encode_cache[sentence] = tokens\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def _get_subwords(self, corpus, vocab_size):\n",
        "        # Count character frequencies\n",
        "        char_counts = Counter(corpus)\n",
        "\n",
        "        # Initialize subwords\n",
        "        subwords = list(char_counts.keys())\n",
        "\n",
        "        # Build vocabulary using unigram language model\n",
        "        while len(subwords) < vocab_size:\n",
        "            # Compute frequencies of all subword pairs\n",
        "            pairs = Counter()\n",
        "            for word in corpus.split():\n",
        "                for i in range(len(word) - 1):\n",
        "                    pairs[word[i:i+2]] += 1\n",
        "\n",
        "            # Find the most frequent subword pair\n",
        "            most_common_pair = pairs.most_common(1)[0][0]\n",
        "\n",
        "            # Merge the most frequent subword pair\n",
        "            new_subword = most_common_pair[0] + most_common_pair[1]\n",
        "            subwords.append(new_subword)\n",
        "\n",
        "            # Replace all occurrences of the most frequent subword pair with the new subword\n",
        "            corpus = corpus.replace(most_common_pair, new_subword)\n",
        "\n",
        "        return subwords\n",
        "\n",
        "    def _get_subword_ids(self, word):\n",
        "        # Initialize variables\n",
        "        subword_ids = []\n",
        "        start = 0\n",
        "        end = len(word)\n",
        "\n",
        "        while start < end:\n",
        "            # Find the longest matching subword\n",
        "            subword = None\n",
        "            for i in range(end, start, -1):\n",
        "                sub = word[start:i]\n",
        "                if sub in self.vocab:\n",
        "                    subword = sub\n",
        "                    break\n",
        "\n",
        "            # If no matching subword is found, add the unknown token\n",
        "            if subword is None:\n",
        "                subword = \"<unk>\"\n",
        "\n",
        "            # Add the subword id to the list of subword ids\n",
        "            subword_id = self.vocab[subword]\n",
        "            subword_ids.append(subword_id)\n",
        "\n",
        "            # Update variables\n",
        "            start = i\n",
        "            i += 1\n",
        "\n",
        "        return subword_ids\n"
      ],
      "metadata": {
        "id": "2KITjD7bes9e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}