{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **what are different types of tokenizers currently using in nlp models**"
      ],
      "metadata": {
        "id": "k0xqG2PUqLQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **There are several types of tokenizers that are commonly used in NLP models. Here are some of the most popular ones:**"
      ],
      "metadata": {
        "id": "bL8C08bfqO0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **White space tokenizer:**"
      ],
      "metadata": {
        "id": "gF1Zft1NqaGF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **This tokenizer splits text into tokens based on white space characters such as spaces, tabs, and newlines.**"
      ],
      "metadata": {
        "id": "QwfVXazpqggh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **It is the simplest type of tokenizer and is often used for tasks such as counting word frequencies.**"
      ],
      "metadata": {
        "id": "0rhd-7OMqjwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Punctuation tokenizer:**"
      ],
      "metadata": {
        "id": "WfSBEguRqvgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **This tokenizer splits text into tokens based on punctuation marks such as periods, commas, and semicolons.**"
      ],
      "metadata": {
        "id": "TD5nsVZuq1tH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **It is useful for tasks such as sentence segmentation.**"
      ],
      "metadata": {
        "id": "myaH8F_rq71M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Rule-based tokenizer:**"
      ],
      "metadata": {
        "id": "7dgnXWslrELE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **This tokenizer uses a set of rules to split text into tokens**"
      ],
      "metadata": {
        "id": "-n2IYVB-rMqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The rules may be based on patterns in the text, such as prefixes or suffixes, or on specific characters or words.**"
      ],
      "metadata": {
        "id": "FXf8iq5hrP7g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Statistical tokenizer:**"
      ],
      "metadata": {
        "id": "12uwWV6lrmIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Statistical tokenizers can handle more complex tokenization tasks, such as handling contractions and compound words**"
      ],
      "metadata": {
        "id": "bqKab_9vvIxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Byte Pair Encoding (BPE) tokenizer:**"
      ],
      "metadata": {
        "id": "oyo8wcVpvWlp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **This tokenizer is often used for subword tokenization, which breaks down words into smaller subword units.**"
      ],
      "metadata": {
        "id": "uMyxYYzXvdrq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BPE works by iteratively merging the most frequent pairs of characters or subwords in the text until a desired vocabulary size is reached.**"
      ],
      "metadata": {
        "id": "6hNJWFi7vmSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BPE is widely used in deep learning models for machine translation and language modeling.**"
      ],
      "metadata": {
        "id": "OFmPN9umvsCC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **WordPiece tokenizer:**"
      ],
      "metadata": {
        "id": "fIyfUKNCv1EL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **This tokenizer is similar to BPE, but instead of merging character pairs, it merges entire words or subwords.**"
      ],
      "metadata": {
        "id": "AbZnPYhBv60Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **WordPiece is often used in pretraining models such as BERT and GPT-2.**"
      ],
      "metadata": {
        "id": "FElpprZiwFOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SentencePiece tokenizer:**"
      ],
      "metadata": {
        "id": "4HnioHOkwNsZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **This tokenizer is an unsupervised subword tokenizer that can handle multiple languages and character sets.**"
      ],
      "metadata": {
        "id": "_cGUygRXwVGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The SentencePiece tokenizer divides input text into a sequence of tokens, which are subsequences of the original text.**"
      ],
      "metadata": {
        "id": "zA9tAocLxcu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The tokenizer can be trained on a corpus of text using an unsupervised learning algorithm, and it generates a vocabulary of subword units that can be used to encode any text in that language.**"
      ],
      "metadata": {
        "id": "gMChUaG5xuct"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C41e9c7gqBJr"
      },
      "outputs": [],
      "source": []
    }
  ]
}