{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrefkquSBrUSHJ4Kh0bB7W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavansai26/tokenizers-in-nlp/blob/main/byte_pair_encoding_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Byte Pair Encoding (BPE) tokenizer is a subword tokenization technique commonly used in Natural Language Processing (NLP) to split words into smaller units called subwords. It is an unsupervised learning algorithm that iteratively merges the most frequent pairs of consecutive bytes in a corpus until a specified number of subwords is obtained."
      ],
      "metadata": {
        "id": "1F0ut6SY38W3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BPE tokenizer is used in various NLP tasks such as text classification, machine translation, and speech recognition. It is particularly useful when dealing with out-of-vocabulary (OOV) words that do not appear in the training data, as it allows the model to recognize similar subwords and generalize to unseen words."
      ],
      "metadata": {
        "id": "_38TpJ333_gU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The advantages of using BPE tokenizer are:\n",
        "\n"
      ],
      "metadata": {
        "id": "7h7pj1H44CRN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Vocabulary size reduction: BPE can reduce the vocabulary size and handle OOV words by breaking down complex words into smaller subwords that can be seen in the training data.\n",
        "\n",
        "2. Improved model performance: By using subwords, BPE can capture the underlying semantic meaning of complex words that may have different variations or inflections, resulting in improved model performance.\n",
        "\n",
        "3. Language agnostic: BPE can be applied to any language and does not require any prior knowledge of the language.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "91Ls581_4FBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The disadvantages of using BPE tokenizer are:\n",
        "\n"
      ],
      "metadata": {
        "id": "dytTP58z4R4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Increased preprocessing time: BPE requires preprocessing the text to learn the subword vocabulary, which can be time-consuming, especially for large datasets.\n",
        "\n",
        "2. Increased computational complexity: Using subwords increases the number of tokens in the vocabulary, which can increase the computational complexity of the model.\n",
        "\n",
        "3. Limited interpretability: Subwords do not always correspond to meaningful linguistic units, making it difficult to interpret the model's predictions.\n",
        " \n",
        "\n"
      ],
      "metadata": {
        "id": "9AOszAsa4UVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a step-by-step explanation of how the Byte Pair Encoding (BPE) tokenizer works:"
      ],
      "metadata": {
        "id": "1wNfyrWI4ocQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Initialize the vocabulary: The BPE tokenizer starts with a set of initial tokens, which can be individual characters, words, or any other sequence of units.\n",
        "\n",
        "2. Count the pairs of units: The BPE tokenizer counts the frequency of each pair of consecutive units in the training corpus, such as \"a b\", \"b c\", \"c d\", etc.\n",
        "\n",
        "3. Merge the most frequent pair: The BPE tokenizer merges the most frequent pair of units into a new token and updates the vocabulary. For example, if the most frequent pair is \"e s\", the tokenizer would merge them into a new token \"es\" and update the vocabulary accordingly.\n",
        "\n",
        "4. Repeat the process: The BPE tokenizer repeats steps 2 and 3 until a predetermined number of subwords is obtained or until the corpus has been fully processed.\n",
        "\n",
        "5. Tokenize the text: Once the vocabulary has been learned, the BPE tokenizer can be used to tokenize new text by splitting the words into subwords that appear in the learned vocabulary. For example, the word \"butterfly\" may be tokenized into \"butt\", \"er\", and \"fly\", which are subwords that appear in the vocabulary.\n",
        "   \n",
        "\n"
      ],
      "metadata": {
        "id": "6nk8Gm_Z4qMb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fot5oOWc34vH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "here's an implementation of the Byte Pair Encoding (BPE) tokenizer using TensorFlow and Python:"
      ],
      "metadata": {
        "id": "y1RlnRkh5nU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import collections\n",
        "import re\n",
        "\n",
        "# Define a function that counts the frequency of pairs of consecutive symbols in a given vocabulary\n",
        "\n",
        "#This function get_stats(vocab) takes in a vocabulary vocab and returns a dictionary of pairs of consecutive symbols (in this case, individual characters and the special end-of-word symbol </w>) and their corresponding frequency counts. \n",
        "#The function first initializes an empty dictionary pairs using collections.defaultdict(int). It then iterates over each word and its frequency in the input vocabulary using vocab.items(), \n",
        "#splits the word into a list of symbols using word.split(), and then iterates over all pairs of consecutive symbols using range(len(symbols)-1). \n",
        "#For each pair, it increments its frequency count in the pairs dictionary using the syntax pairs[symbols[i],symbols[i+1]] += freq. Finally, it returns the resulting pairs dictionary.\n",
        "\n",
        "def get_stats(vocab):\n",
        "    pairs = collections.defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "            pairs[symbols[i],symbols[i+1]] += freq\n",
        "    return pairs\n",
        "\n",
        "# Define a function that merges the most frequent pair of symbols in a given vocabulary\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "This function merge_vocab(pair, v_in) takes in a pair of symbols pair and a vocabulary v_in, and returns a new merged vocabulary v_out. \n",
        "The function first initializes an empty dictionary v_out to store the merged vocabulary. \n",
        "It then creates a regular expression pattern to match the given pair of symbols, using re.escape() to escape any special characters in the pair \n",
        "and join() to concatenate the pair into a string separated by spaces. The resulting pattern is then compiled into a regular expression object p using re.compile(). \n",
        "The regular expression pattern is constructed to match the given pair of symbols only if it occurs at the beginning of a word or immediately after whitespace.\n",
        "\n",
        "The function then iterates over each word in the input vocabulary v_in and replaces all occurrences of the given pair of symbols with a new symbol created by concatenating the two symbols using join(). \n",
        "The sub() method of the regular expression object p is used to replace all occurrences of the pattern with the new symbol. \n",
        "The resulting modified word is then added to the new merged vocabulary v_out using the same frequency count as the original word in v_in.\n",
        "\n",
        "Finally, the function returns the resulting merged vocabulary v_out.\n",
        "\n",
        "\"\"\"\n",
        "def merge_vocab(pair, v_in):\n",
        "    v_out = {}\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "    for word in v_in:\n",
        "        w_out = p.sub(''.join(pair), word)\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out\n",
        "\n",
        "# Define the main function that performs the Byte Pair Encoding\n",
        "def byte_pair_tokenize(vocab, num_merges):\n",
        "    for i in range(num_merges):\n",
        "        pairs = get_stats(vocab)\n",
        "        best = max(pairs, key=pairs.get) # Find the most frequent pair of symbols\n",
        "        vocab = merge_vocab(best, vocab) # Merge the most frequent pair of symbols\n",
        "    return vocab\n",
        "\n",
        "# Example usage\n",
        "corpus = ['hello world', 'hello tensorflow', 'tensorflow is awesome']\n",
        "\n",
        "# Define a vocabulary from the corpus by counting the frequency of individual characters and adding the '</w>' symbol\n",
        "vocab = collections.defaultdict(int)\n",
        "for sentence in corpus:\n",
        "    for word in sentence.split():\n",
        "        vocab[' '.join(list(word)) + ' </w>'] += 1\n",
        "\n",
        "num_merges = 10 # Specify the number of merges to perform\n",
        "bpe_vocab = byte_pair_tokenize(vocab, num_merges) # Perform Byte Pair Encoding\n",
        "print(bpe_vocab) # Print the resulting subword vocabulary\n"
      ],
      "metadata": {
        "id": "_rB74ElW5n3P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}