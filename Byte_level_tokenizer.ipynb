{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOGxnzwh3hha3mZgzdeRBn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavansai26/tokenizers-in-nlp/blob/main/Byte_level_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Byte-level tokenizer is a type of tokenizer that tokenizes text at the byte level, which means that it splits text into individual bytes or characters, including spaces and special characters."
      ],
      "metadata": {
        "id": "MZ5UUBry-XLx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Byte-level tokenization can be useful in certain NLP applications where it is necessary to preserve the exact sequence of bytes or characters, such as in text compression, cryptography, or data analysis. It is also sometimes used as a preprocessing step for other types of tokenization."
      ],
      "metadata": {
        "id": "fm8VwUUC-cQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some advantages of byte-level tokenization are:\n",
        "\n"
      ],
      "metadata": {
        "id": "BeIueyJp-o4R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Flexibility: Byte-level tokenization can handle a wide range of text formats and languages.\n",
        "\n",
        "2. Granularity: Byte-level tokenization can capture fine-grained distinctions between different characters or bytes, which can be useful for some applications.\n",
        "\n",
        "3. Control: Byte-level tokenization gives users more control over the tokenization process, as they can define their own rules for how to split text.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8ZH1Ir2R-rwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, there are also some disadvantages to byte-level tokenization, including:\n",
        "\n"
      ],
      "metadata": {
        "id": "OICA7eUe-965"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Ambiguity: Byte-level tokenization can sometimes create ambiguous tokens, especially for multibyte characters or special characters that have different representations in different encoding schemes.\n",
        "\n",
        "2. Complexity: Byte-level tokenization can be more complex and time-consuming than other types of tokenization, especially for large datasets.\n",
        "\n",
        "3. Inefficiency: Byte-level tokenization can result in a large number of tokens, which can be inefficient for some NLP models or applications.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rzkoBr2t_F7H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Byte-level tokenization is a type of tokenizer that splits text into individual bytes or characters, including spaces and special characters. It is a very basic and flexible form of tokenization that can handle a wide range of text formats and languages.\n",
        "\n",
        "Here's an example of how byte-level tokenization works:\n",
        "\n",
        "Let's say we have the following text: \"Hello, world!\"\n",
        "\n",
        "In byte-level tokenization, this text would be split into individual bytes or characters, like this:\n",
        "\n",
        "\"H\", \"e\", \"l\", \"l\", \"o\", \",\", \" \", \"w\", \"o\", \"r\", \"l\", \"d\", \"!\"\n",
        "\n",
        "As you can see, each individual byte or character in the text has become a separate token. This is the simplest form of tokenization and can be useful in certain NLP applications where it is necessary to preserve the exact sequence of bytes or characters."
      ],
      "metadata": {
        "id": "gR4nJ3aU_TwJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhX87W7b8BSH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the input text\n",
        "text = \"Hello, world!\"\n",
        "\n",
        "# Convert the text to bytes using UTF-8 encoding\n",
        "bytes_text = text.encode('utf-8')\n",
        "\n",
        "# Create a TensorFlow dataset from the bytes\n",
        "dataset = tf.data.Dataset.from_tensor_slices(bytes_text)\n",
        "\n",
        "# Define the byte-level tokenizer function\n",
        "def tokenize_byte(byte):\n",
        "    return byte\n",
        "\n",
        "# Tokenize the bytes using the tokenizer function\n",
        "tokenized_dataset = dataset.map(tokenize_byte)\n",
        "\n",
        "# Print the resulting tokens\n",
        "for token in tokenized_dataset:\n",
        "    print(token.numpy().decode('utf-8'))\n"
      ]
    }
  ]
}