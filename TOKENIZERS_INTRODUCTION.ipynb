{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **WHAT IS TOKENIZATION**"
      ],
      "metadata": {
        "id": "Lzugn5blTdnO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **In Natural Language Processing (NLP), tokenization is the process of breaking down text into smaller units called tokens.**"
      ],
      "metadata": {
        "id": "jZxapk6sTkGX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization is a fundamental step in many NLP tasks such as machine translation, sentiment analysis, and text classification. Once text has been tokenized, it can be analyzed further using other NLP techniques**"
      ],
      "metadata": {
        "id": "kK1J1yzsaocp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **WHAT ARE THE DIFFERENT TYPE OF TOKENS**"
      ],
      "metadata": {
        "id": "189kdxnZUEJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **tokens can be individual words, phrases, or even sentences, depending on the level of granularity required for the task at hand.**"
      ],
      "metadata": {
        "id": "LLyHYacaURQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **WHAT ARE THE MAIN TYPES OF TOKENIZERS**"
      ],
      "metadata": {
        "id": "X9LWGb6dUrd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **There are various types of tokenizers available**\n"
      ],
      "metadata": {
        "id": "lDxrcUmAU9lG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. rule-based tokenizers**\n",
        "# **2. statistical tokenizers**\n",
        "# **3. machine learning-based tokenizers.**"
      ],
      "metadata": {
        "id": "HP7Ti4tDVEHq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **WHAT ARE Rule-Based tokenizers**"
      ],
      "metadata": {
        "id": "tSA1dQN5WsES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **a.Rule-based tokenizers follow a set of predefined rules to split text into tokens.**\n",
        "\n",
        "# **b.These rules may be based on patterns in the text, such as whitespace, punctuation, or other delimiters**"
      ],
      "metadata": {
        "id": "EELMFFQSXLj2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **what are statistical tokenizers**"
      ],
      "metadata": {
        "id": "1lpyafKhY2qT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Statistical tokenizers use probabilistic models to identify the most likely boundaries between tokens.**"
      ],
      "metadata": {
        "id": "WfZBUm_wZK3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **These models are trained on large amounts of text and learn to recognize patterns that are indicative of word boundaries.**"
      ],
      "metadata": {
        "id": "Oyuwtl76ZqLw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **what are machine learning based tokenizers**"
      ],
      "metadata": {
        "id": "QjTcwWSjZPWe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Machine learning-based tokenizers use deep learning models to learn the structure of language and identify tokens.**"
      ],
      "metadata": {
        "id": "GmSfE5-DZiXw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **These models are typically trained on large amounts of text using neural network architectures such as convolutional neural networks (CNNs) or recurrent neural networks (RNNs).**"
      ],
      "metadata": {
        "id": "HnL4ZwcNaNEk"
      }
    }
  ]
}